-
- # Week 1: Introduction to AI and Intelligent Agents
- ## Unit Introduction
- ### Teaching Team
- **Lecturer**: Julian Gutierrez
	- Office: 29 Ancora Imparo Way, Office 334, Clayton
	- Email: Julian.Gutierrez@monash.edu
	- Consultation time: Wednesday 4-5PM
- **Tutors**:
	- Bruce Y. Chen (Head tutor) - Bruce.Ying.Chen@monash.edu
	- Fan Yang - Fan.Yang1@monash.edu
	- Trang Vu - Trang.Vu1@monash.edu
	- Xiaoyun Hu - Xiaoyun.Hu@monash.edu
	- Yifei Hu - Yifei.Hu@monash.edu
- ### Unit Structure
  
  **Lectures**
- One 2-hour lecture per week
- Time and place: Wednesday at 2-4 PM
  
  **Tutorials (9) and Revisions (2)**
- 2 hours per week, compulsory, starting in week 2
- Conducted by tutors
- Day and time options:
	- Wednesday 7-9 PM
	- Thursday 10-12 noon
	- Friday 5-7 PM
- Students should prepare for tutorials
- ### Unit Objectives
- Principles and theoretical basis of AI
	- Learn about the main AI theories and techniques
- Agent-based approach for building AI systems
- Use of AI to develop computational systems
- ### Topics Covered
  1. **LN1**: Introduction to AI
  2. **LN2**: Intelligent Agents
  3. **LN3**: Problem solving as search
  4. **LN4**: Knowledge representation - Predicate calculus
  5. **LN5**: Reasoning under uncertainty:
	- Probabilistic reasoning
	- Bayesian networks
	  6. **LN7**: Machine learning:
	- Supervised
	- Unsupervised
- ### Timetable
  
  | Week-Date | Lecture Notes | Tutorials | Assessment |
  |-----------|---------------|-----------|------------|
  | 1 – Aug 5 | Intro LN1 LN2 | | |
  | 2 – Aug 12 | LN3 (2) | T01: LN2 | |
  | 3 – Aug 19 | LN3 (2) | T02: LN3 | |
  | 4 – Aug 26 | LN3 (2) | T03: LN3 | Release (Ass1) |
  | 5 – Sep 2 | LN4 (2) | T04: LN3 | |
  | 6 – Sep 9 | LN4 (2) | T05: LN4 | |
  | 7 – Sep 16 | LN5 (2) | T06: LN4 | Due (Ass1) |
  | Mid-semester break | | | |
  | 8 – Oct 7 | LN5 (2) | Revision | Release (Ass2) |
  | 9 – Oct 14 | LN5 (2) | T07: LN5 | |
  | 10 – Oct 21 | LN6 (2) | T08: LN5 | |
  | 11 – Oct 28 | LN6 (2) | T09: LN6 | Due (Ass2) |
  | 12 – Nov 4 | LN6 (2) | Revision | |
- ### Assessment
  
  **Assignments (40%)**
- Assignment 1: Due on Sep 20 (20%) – Week 7
- Assignment 2: Due on Nov 1 (20%) – Week 11
  
  **Final Exam: 2 hours (60%)**
- Sometime in Nov 16-20; 23-27
- Exam results published on Dec 17
  
  **Resources**
- Available in Moodle
- ### Assessment: Marks and Hurdles
  
  **To pass FIT5047:**
- Your marks must average to at least 50
- You must pass the 40-40 hurdle:
	- Average of assessment ≥ 40%
	- Final exam ≥ 40% (you can sit the final exam even if you have failed the assessment hurdle)
	  
	  > **Important**: Failure to meet any hurdle will result in a maximum mark of 49N
- ### Attending a Different Tutorial
  
  If you can't attend your tutorial (with legitimate reason), you need to receive written permission from the Admin tutor (Bruce Y. Chen) to attend a different tutorial.
  
  **To obtain this permission**, email Bruce.Ying.Chen@monash.edu with the subject line: "Change of tutorial" and include:
- NAME:
- ID NUMBER:
- REGULAR TUTORIAL: (time, room and date)
- PROPOSED REPLACEMENT TUTORIAL: (time, room and date)
- REASON FOR CHANGE OF TUTORIAL
- SCANNED CERTIFICATION
- ### Communication with the Teaching Team
  
  **Emails about**: unit, lectures, tutorials, technical questions, personal matters, permissions, other
- **Unit, lectures, or personal matters**: Email the Lecturer
- **Tutorials**: Email your Tutor
- **Technical questions**: Use Moodle's "General Discussion" Forum
- **Permissions (Tutorials)**: Email Bruce Y. Chen
- **Any other**: Email the Lecturer
  
  **Use of Moodle:**
- The General Discussion forum will be used to discuss technical questions
- This forum will be checked by the teaching team once a day (from 12 to 1PM), and not during weekends
- The teaching team will send announcements via Moodle using the Unit Announcement forum
- ### Resources: Moodle
- Unit information
- Lecture notes (i.e., Lecture slides)
- Tutorial notes
- Forums:
	- **Discussion**: for technical questions (TeachingTeam ↔ Students)
	- **Announcements**: for class announcements (TeachingTeam → Students)
- ### Resources: Reading Material
  
  **Prescribed textbook:**
- Russell, S. and Norvig, P. (2010), *Artificial Intelligence: A Modern Approach* (3rd ed), Prentice Hall
  
  **Supplementary textbooks:**
- May be used as additional support material
- ### Seeking Assistance as a Preventive Measure
- **Study difficulties**: Discuss any difficulties you are experiencing with your course leader, unit coordinator, lecturer, head tutor, or tutor
- **Study methods, language skills and work presentation**: Language and learning online can help you with these
	- http://www.monash.edu.au/lls/llonline/
- **Student life and support services**, including health services, support and services, and clubs and sports
	- http://monash.edu/students/support/
- ### Disability Support Services
  
  **Do you have a disability, medical or mental health condition that impacts on your study?**
  
  Disability Services provides a range of services for registered students including:
- Note takers and Auslan interpreters
- Readings in alternative formats
- Adaptive equipment and software
- Alternative arrangements for exam
  
  **Further information and details on how to register:**
- Email: disabilitysupportservices@monash.edu
- Phone: 03 9905 5704
- Web: https://www.monash.edu/disability/contact-us
- https://www.monash.edu/disability/for-student
- ### Cheating and Plagiarism
- When you submit a report you sign that your work is your own, and not someone else's
- If we find unexplained similarities between assignments, there are consequences:
	- These range from disallowing a question in an assignment to failing the unit completely
	- If you are found guilty, a report regarding plagiarism or cheating is entered in your permanent academic record
	  
	  ---
- ## Lecture Topic 1 (LN1): Introduction to AI
- ### What is Intelligence?
  
  An entity is intelligent if:
- It can **communicate**
- It has **internal knowledge**
- It has **world knowledge**
- It has **intentions and plans**, which should be consistent with such intentions
- It has **creativity**
- ### FIT5047: Fundamentals of AI
  
  This unit introduces the main problems and approaches to designing AI systems including:
- Automated search methods
- Knowledge representation and reasoning
- Reasoning under uncertainty
- Machine learning paradigms
- ### What is Artificial Intelligence (AI)?
- **AI is the study of mental faculties through the use of computational models** (Charniak and McDermott, 1985)
- **AI is the study of how to make computers do things that (at the moment) humans do (better)** (Rich and Knight, 1991)
- **AI is the science of making computers act like the ones in the movies** (Anonymous)
- ### Goals of AI Practitioners
- Find out about the nature of intelligence
- Build intelligent machines
  
  **Build systems that:**
  
  |  | **Humanly** | **Rationally** |
  |---|---|---|
  | **Thinking** | Think like humans | Think rationally |
  | **Acting** | Act like humans | Act rationally |
- ### Acting Humanly: The Turing Test (I)
  
  **Turing test (1950)**
  
  ```
  Human Interrogator
        ↓
    [AI System] ← → [Human]
  ```
- ### Acting Humanly: The Turing Test (II)
  
  **Turing (1950)**
- **Can machines think?** → **Can machines behave intelligently?**
- **Operational test for intelligent behaviour**: the Imitation Game
  
  **Pros (✓):**
- Suggested major components of AI: knowledge, reasoning, learning, communication/understand
  
  **Cons (✗):**
- Not reproducible, not constructive, and not amenable to mathematical analysis
- ### Acting Rationally
- **Rational behaviour**: doing the best/right thing
	- The right thing: that which is expected to maximize goal achievement, given the available information
	  
	  > **Aristotle (Nicomachean Ethics)**: Every art and inquiry, and similarly every action and pursuit, is thought to aim at some good
- ### Rational Agents
- An **agent** is an entity that perceives and acts
- Abstractly, an agent is a function from percept histories to actions: $f: P^* \to A$
- For any given class of environments and tasks, we seek the agent(s) with the best performance
- **Caveat**: computational limitations make perfect rationality unachievable → **bounded rationality**
	- Design the best program for a given machine's resources
- ### Autonomous Agency
  
  **Autonomy**
- Ability to operate independently
  
  **Agency**
- Having internal goal structure and external behaviour which generally serves to satisfy a goal structure
  
  **Requirements of autonomous agency:**
- Pragmatics
- Generalization and specialization
- Incremental learning
- Goal-driven learning
- Defeasibility (ability to change its mind)
- Ability to deal with uncertainty
-
- ### Problems Attacked in AI
- Representation
- Decoding
- Inference
- Controlling combinatorial explosion
- Planning
- Indexing
- Prediction and recovery
- Dynamic modification
- Generalization
- Curiosity
- Creativity
- ### Subfields of AI
  
  **Methods:**
- **Knowledge Representation** (Logic, Bayes Nets, Semantic nets)
- **Reasoning** (Logic, Bayes Nets, Spreading activation)
- **Planning / decision making** (Goal-based planning, MDPs)
- **Search** (A*, simulated annealing, genetic algorithms)
- **Machine Learning** (Artificial neural networks, decision trees, Naïve Bayes, Reinforcement learning)
  
  **Applications:**
- Decision support/making systems
- Data mining/science
- Game playing
- Robotics, Vision, NLP
- Optimisation
- Nowadays, pretty much anything!
- ### History of AI (I)
- **1943**: Perceptrons/Neural nets/Connectionism (McCulloch and Pitts 1943, Rosenblatt 1957)
- **1950s**: Machine translation
- **1950**: Turing initiated AI as a research area
- **1956**: Dartmouth conference: Birth of AI
	- Origin of Artificial Intelligence as a name
- **1963**: Checkers playing (Samuel 1963)
- **1963**: Theorem Prover (Newell 1963)
	- GPS - General Problem Solver (Newell, Shaw & Simon)
	- Basic technique: Means-ends analysis
- **1964**: Bayesian inference applied to authorship attribution (Mosteller and Wallace 1964)
- **1965**: Robinson's complete algorithm for logical reasoning
- ### History of AI (II)
- **1966-74**: AI has a reality check: no world knowledge and no scaling up (high computational complexity)
- **1974**: Neural networks research almost disappears
- **1969-79**: Knowledge-based systems
- **1980**: AI becomes an industry:
	- Expert systems, vision systems, robotics
- **1986**: Neural networks return to popularity
- **1987**: Probability is back; increase in technical depth
	- "Nouvelle AI": ALife, Genetic Algorithms, soft computing
- **1995**: Increase interest in agent-based systems
- **2001**: Big data, Deep learning
- ### State of the Art (I)
- **Autonomous agents**
	- Smart spaces/ambient intelligence
	- Smart personal assistants
- **Data mining** (business intelligence)
- **Machine learning applications**
	- e.g., spam fighting, disease diagnosis (probabilistic expert systems)
- **Google's search engine** (page ranking)
- **Recommender systems** (directed advertising)
- ### State of the Art (II)
- **Autonomous planning and scheduling** (1991, 1999, 2004, 2008)
- **Robotic vehicles – autonomous driving** (1995, 2006, 2007, now)
- **Robotics** – Roomba (2002), packBot (2002)
- **Game playing** – Deep Blue defeated the world chess champion Garry Kasparov (1997), AlphaGo defeated the world Go champion Lee Sedol (2016)
- **Statistical machine translation** (2007)
- **Winning Jeopardy** – Watson (2011)
- **Speech recognition** – in restricted domains
- ### What We Will Do Here
- Learn what some of the key problems in AI are
- Learn some key strategies for solving them
- Learn about typical applications
- ### Reading
  
  **Main textbook:**
- Russell, S. and Norvig, P. (2010), *Artificial Intelligence – A Modern Approach* (3rd ed), Prentice Hall, Chapter 1
  
  **Other references:**
- W.S. McCulloch and W. Pitts (1943) A logical calculus of the ideas immanent in nervous activity. Bull Math Biophysics, 5, 115-137
- A. Turing (1950) Computing machinery & intelligence. Mind, 59, 433-460. Reprinted many times (e.g., Boden (ed) Philosophy of AI, Oxford, 1990)
- F. Rosenblatt (1957) The Perceptron. Report 85-460-1 Cornell Aeronautical Lab
- M. Minsky and S. Papert (1969) Perceptrons. MIT
- A. Newell & H.A. Simon (1976) Computer science as empirical inquiry. Communications of the ACM, 19. Reprinted in Boden
- M. Boden (1977), Artificial Intelligence and Natural Man. Basic Books Inc.
  
  ---
- ## Lecture Topic 2 (LN2): Intelligent Agents
- ### Outline
- Agents and environments
- Rationality
- PEAS (Performance measure, Environment, Actuators, Sensors)
- Environment types
- Agent types
- ### Agents
- An **agent** is anything that can be viewed as perceiving its environment using **sensors**, and acting upon that environment via **actuators**
  
  **Human agent:**
- Eyes, ears and other organs for sensors
- Hands, legs, mouth and other body parts for actuators
  
  **Robotic agent:**
- Cameras and infrared range finders for sensors
- Various motors for actuators
- ### Agents and Environments
- The **agent function** maps from percept histories to actions: $f: P^* \to A$
- The **agent program** runs on the physical architecture to produce $f$
- **agent = architecture + program**
- ### Example: Agent in a Vacuum-Cleaner World
  
  **Percepts:** location and contents, e.g., [A, Dirty]
  
  **Actions:** Left, Right, Suck
  
  **Program:**
  ```
  If status=Dirty return Suck
  ElseIf Location=A return Right
  ElseIf Location=B return Left
  ```
  
  > **Question**: Is this a rational agent?
- ### Rationality and Rational Agents
  
  **Rationality depends on:**
- Performance measure
- The agent's prior knowledge of the environment
- The actions that the agent can perform
- The percept sequence to date
  
  **Definition:**
  
  > For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and the agent's built-in knowledge
- ### Rational Autonomous Agents
- **Rationality is NOT omniscience**
- Agents can perform actions to modify future percepts in order to obtain useful information → exploration, learning
- An agent is **autonomous** if its behavior is determined by its own experience
- ### Task Environment – PEAS
  
  To design a rational agent, we must specify the **Task environment**
  
  **PEAS:**
- **P**erformance measure
- **E**nvironment
- **A**ctuators
- **S**ensors
- ### PEAS – Example (I): Automated Taxi Driver
  
  **Performance measure:**
- Safe, fast, legal, comfortable trip, minimize fuel consumption, maximize profit
  
  **Environment:**
- Road types, road contents, customers, operating conditions
  
  **Actuators:**
- Control over the car, interfaces for informing other vehicles and informing passengers
  
  **Sensors:**
- Cameras, sonar, speedometer, GPS, odometer, engine sensors, interface for receiving information from other vehicles and passengers (e.g., speech recognizer)
- ### PEAS – Example (II): Internet Shopping Agent
  
  **Performance measure:**
- Cheap, good quality, appropriate product
  
  **Environment:**
- Current internet sites, vendors
  
  **Actuators:**
- Packages that display to user, follow URL, fill in forms
  
  **Sensors:**
- Packages that read HTML pages (text, graphics, scripts)
- ### Environment Types (I)
  
  The environment type largely determines the agent design
- **Fully (partially) observable** – An agent's sensors give it access to the complete state of the environment at all times
- **Known (unknown)** – An agent knows the "laws" of the environment
- **Single (multi) agent** – An agent operating by itself in an environment
- **Deterministic (stochastic)** – The next state is completely determined by the current state and the action executed by the agent
- ### Environment Types (II)
- **Episodic (sequential)** – The agent's experience is divided into atomic episodes. The next episode does NOT depend on previous actions
	- In each episode an agent perceives a percept and performs a single action
- **Static (dynamic)** – The environment is unchanged while an agent is deliberating
- **Discrete (continuous)** – Pertains to number of states, the way time is handled, and number of percepts and actions
	- E.g., states may be continuous, but actions be discrete
- ### Environment Types – Examples
  
  > The real world is partially observable, unknown, multi-agent, stochastic, sequential, dynamic, continuous
  
  | | Sorting laundry | 8-puzzle | Medical diagnosis | Backgammon | Taxi |
  |---|---|---|---|---|---|
  | Observable? | | | | | |
  | Known? | | | | | |
  | Single agent? | | | | | |
  | Deterministic? | | | | | |
  | Episodic? | | | | | |
  | Static? | | | | | |
  | Discrete? | | | | | |
- ### Environments and Methodologies
  
  | | Search | Logical inference | Bayesian networks | Machine learning |
  |---|---|---|---|---|
  | Observable? | ✓ | ✓ | | |
  | Known? | ✓ | ✓ | ✓ | ✗ |
  | Single agent? | | | | |
  | Deterministic? | ✓ | ✓ | ✗ | |
  | Episodic? | | | | |
  | Static? | ✓ | ✓ | ✓ | ✓ |
  | Discrete? | ✓ | ✓ | | |
- ### Agent Functions and Programs
- An agent is specified by its **agent function** which maps percept sequences to actions
- **Aim**: design a program that implements the rational agent function concisely
- ### Agent Types
  
  Based on the function = how actions are selected
  
  | Agent type | Action selected based on |
  |---|---|
  | Simple reflex | current percept |
  | Model based | + internal state (world model) |
  | Goal based | + goal |
  | Utility based | + utility function |
  | Learning | performance element = above agent<br>+ critic<br>+ learning element<br>+ problem generator (exploratory) |
- ### Agent Types: Taxi Example
  
  | Agent type | Action |
  |---|---|
  | Simple reflex | brake when brake-lights of car in front light up |
  | Model based | + remember the roads travelled, time, state |
  | Goal based | + make a plan to reach a destination |
  | Utility based | + quickest with least petrol consumption |
  | Learning<br>performance elem<br>+critic<br>+learning element<br>+problem generator | above agent<br>observes the world & informs learning elem<br>formulates new driving rules based on the feedback from critic + perf agent knowledge<br>might suggest some driving exercises |
- ### How Components of Agent Programs Work?
  
  Depends on the representation of states:
- **Atomic** – each state is indivisible (Search, Game playing, Markov Decision Processes)
- **Factored** – splits each state into attributes, each of which has a value (Propositional logic, Bayesian networks, Machine learning)
- **Structured** – represents how things are related to each other (First-order logic, First-order probability models)
  
  > A more expressive representation can capture everything a less expressive representation can capture, but reasoning and learning is harder
- ### Reading
- Russell, S. and Norvig, P. (2010), *Artificial Intelligence* – Chapter 2
  
  ---
- # Week 2-3: Problem Solving as Search
- ## Lecture Topic 3 (LN3): Solving Problems by Searching
- ### Problem Solving: Learning Objectives
  
  **Problem formulation**
  
  **Control strategies:**
  
  **Tentative:**
- **Uninformed:**
	- Backtracking [Chapter 7]
	- Tree- and Graph search [Chapter 3]
- **Informed:**
	- Greedy best-first search, A, A* [Chapter 3]
	  
	  **Irrevocable:**
- **Informed:**
	- Hill climbing, Local beam search, Simulated annealing, Genetic algorithms [Chapter 4]
	  
	  **Adversarial search algorithms [Chapter 5]:**
- Optimal decisions
- Minimax, α-β pruning
- ### Assumptions About the Environment
- Observable
- Known
- Single/multi agent
- Deterministic
- Sequential
- Static/dynamic
- Discrete
- ### Problem-Solving Agents
  
  ```
  Function Simple-Problem-Solving-Agent(percept) returns seq
  persistent: state – description of current world state
           seq – action sequence (initially null)
           goal – a goal
           problem – a problem formulation
  
  state ← UpdateState(state, percept)
  goal ← FormulateGoal(state)
  problem ← FormulateProblem(state, goal)
  seq ← Search(problem)
  return seq
  ```
- ### Example: Romania
- On holiday in Romania; currently in Arad
- Flight leaves tomorrow from Bucharest
  
  **Formulate goal:**
- be in Bucharest
  
  **Formulate problem:**
- states: various cities
- actions: drive between cities
  
  **Find solution:**
- sequence of cities, e.g., Arad, Sibiu, Fagaras, Bucharest
  
  ---
- ## Problem Formulation
- ### Problem Formulation (I)
  
  **Problem formulation** comprises decisions about:
- which properties of the world matter
- which actions are possible
- how to represent world states and actions
  
  > Abstracting away from unnecessary detail is a key
  > → It can drastically reduce the size of the state/search space
- ### Problem Formulation (II)
  
  **Basic constituents:**
- States, Goals, Actions, Constraints
  
  **Definitions:**
- **State space** – the set of all states reachable from the initial state by any sequence of actions
- **Path in the state space** – any sequence of actions leading from one state to another
  
  **Representing a problem:**
- Initial state
- Operators (Actions) and transition model
- Constraints
- Goal test
- Path cost function
  
  > A solution is a sequence of actions leading from the initial state to a goal state
- ### Problem Formulation: Example
  
  1. **initial state**, e.g., "at Arad"
  
  2. **actions**
	- e.g., {Go(Sibiu), Go(Timisoara), …}
	- **transition model**
		- e.g., Result(In(Arad), Go(Zerind)) → In(Zerind)
		  
		  3. **constraints** – nil
		  
		  4. **goal test** can be
	- explicit, e.g., In(Bucharest)
	- implicit, e.g., Checkmate(x)
	  
	  5. **path cost** (additive)
	- e.g., sum of distances, number of actions executed
	- $c(s,a,s')$ is the step cost of taking action $a$ at state $s$ to reach state $s'$ (assumed to be ≥ 0)
- ### Problem Formulation – 8 Puzzle (I)
  
  ```
  Start:          End:
  5 4 _          1 2 3
  6 1 8          8 _ 4
  7 3 2          7 6 5
  ```
- ### Problem Formulation – 8 Puzzle (II)
  
  **States:**
- Location of each of the 8 tiles in one of the 9 squares
  
  **Operators:**
- Possible moves of blank tile
  
  **Constraints:**
- A tile cannot move out of bounds
  
  **Goal test:**
- Have we reached the goal configuration?
  
  **Path cost:**
- If we want to minimize the number of steps, then cost of 1 per step
- ### Problem Formulation – Vacuum World
  
  **States:**
- 8 states shown
  
  **Operators:**
- Left, right, suck
  
  **Constraints:**
- Can't leave the space
  
  **Goal test:**
- States 7 and 8
  
  **Path cost:**
- Each action costs 1
- ### Problem Formulation: Missionaries and Cannibals (I)
  
  **Start state:** 3 missionaries & 3 cannibals on one side of a river
  
  **Goal state:** 3 missionaries & 3 cannibals on the other side of the river
  
  **Constraints:**
- There is a boat that carries at most 2 people
- The boat cannot travel empty
- Cannibals should never outnumber missionaries
- ### Problem Formulation: Missionaries and Cannibals (II)
  
  **States:**
- 2-digit code (m,c) represents the number of m and c on start bank; 1 digit code represents boat position
- Initial state (3,3) + boat position
  
  **Operators:**
- 1m1c, 2m, 2c, 1m, 1c
  
  **Constraints:**
- $[(c \leq m) \land (3-c \leq 3-m)] \lor m=3 \lor m=0$
  
  **Goal test:**
- (0,0)
  
  **Path cost:**
- Cost function: Minimize number of crossings
- ### Problem Formulation: Robotic Assembly
  
  **states?** real-valued coordinates of robot joint angles; parts of the object to be assembled
  
  **actions?** continuous motions of robot joints
  
  **constraints?** arm cannot fully rotate up and down
  
  **goal test?** complete assembly
  
  **path cost?** time to execute
- ### Selecting a State Space
- Real world is complex → state space must be abstracted for problem solving
- **(Abstract) state** = set of real states
- **(Abstract) action** = complex combination of real actions
	- e.g., "Arad → Zerind" represents a complex set of possible routes, detours, rest stops, etc
- For guaranteed realizability, any real state ("in Arad") must get to some real state ("in Zerind")
- **(Abstract) solution** = a solution that can be expanded into a set of real paths in the real world
- Each abstract action should be "easier" to perform than solving the original problem
  
  ---
- ## Control Strategies
- ### Classification of Control Strategies
  
  **Tentativeness:**
- **Irrevocable** – no reconsideration
- **Tentative** – with reconsideration
  
  **Informedness:**
- **Uninformed** – decide based only on problem definition
- **Informed** – use guidance on where to look for solutions
  
  | | Irrevocable | Tentative |
  |---|---|---|
  | **Uninformed** | -- | Backtrack, Tree- and Graph-Search (BFS, DFS, DLS, IDS, UCS) |
  | **Informed** | Hill climbing, Local beam search, Simulated annealing, Genetic algorithms | Greedy best-first search, A, A* |
  
  ---
- ## Tentative Search Algorithms: Backtrack, Tree- and Graph-search
- ### Tentative Control Strategies
  
  **Backtracking** – at any point in time, we keep one path only
- If we fail, we go back to the last decision point and erase the failed path
- Backtracking occurs when:
	- we reach a DEADEND state OR
	- there are no more applicable rules OR
	- we generate a previously encountered state description OR
	- an arbitrary number of rules has been applied without reaching the goal
	  
	  **Graphsearch** – we keep track of several paths simultaneously
- Done using a structure called a search tree/graph
- ### Basic Backtrack Algorithm
  
  ```
  Procedure Backtrack(State)
  1. If Goal(State) Then return SUCCEED
  2. If Deadend(State) Then return FAIL
  3. Operators ← ApplicableOps(State)
  4. Loop
   1. If null(Operators) Then return FAIL
   2. Op ← Pop(Operators)
   3. State' ← Op(State)
   4. Path ← Backtrack(State')
   5. If Path=FAIL Then go Loop
   6. Return {Op, Path}
  End
  ```
- ### Backtrack Algorithm
  
  ```
  Procedure Backtrack1(StateList)
  1. State ← First(StateList)
  2. If State ∈ RestOf(StateList) Then return FAIL
  3. If Goal(State) Then return SUCCEED
  4. If Deadend(State) Then return FAIL
  5. If Length(StateList) > Bound Then return FAIL
  6. Operators ← ApplicableOps(State)
  7. Loop
   1. If null(Ops) Then return FAIL
   2. Op ← Pop(Ops)
   3. State' ← Op(State)
   4. StateList' ← {State', StateList}
   5. Path ← Backtrack1(StateList')
   6. If Path=FAIL Then go Loop
   7. Return {Op, Path}
  End
  ```
- ### Backtracking Example – 4 Queens Problem
  
  **Start state:**
- empty chess board
  
  **Goal state:**
- 4 queens placed on chess board
  
  **Constraints:**
- queens don't attack each other
  
  **Operators:**
- place queen on tile (x,y)
  
  **Path cost:** NA
- ### Graphsearch – Definitions
- **Graphsearch** is a means of finding a path in a graph from a node representing the initial state to a node that satisfies the goal condition
  
  **Definitions:**
- **Graph** – set of nodes
- **Arcs** – connect between certain pairs of nodes
- **Directed graph** – formed by arcs directed from one node to another
- $n_i$ is a **child** of $n_k$ if there exists an arc from $n_k$ to $n_i$
- $n_i$ is **accessible** from $n_k$ if there is a path from $n_k$ to $n_i$
- **Expanding a node** – finding all its children
- **Search Problem** – find a path between node s and any member of the goal set {$t_i$} that represents states satisfying the goal condition
- ### Search Tree
- **Tree** – each node has at most one parent
- **Root** of search tree is the initial state
- **Leaves** are states without successors (the "fringe" or "frontier")
- At each step, choose one leaf node to expand
- ### Basic Tree Search Algorithm
  
  ```
  function TREE-SEARCH(problem) returns a solution or failure
  • Initialize the frontier using the initial state of problem
  • Loop
  1. if the frontier is empty then return failure
  2. choose a leaf node and remove it from the frontier
  3. if the node contains a goal state then return the corresponding solution
  4. expand the chosen node, adding the resulting nodes to the frontier
  end
  ```
- ### Implementation: States vs. Nodes
- **state** – a (representation of a) physical configuration
- **node** – a data structure that is part of a search tree
	- includes state, parent node, action, children, path cost g(x), depth
	  
	  **The Expand function:**
- creates new nodes, fills in the various fields
- uses SuccessorFn(Operators) to create the corresponding states
- ### Searching Graphs: Multiple Paths to a Node
- Often search is better represented via graphs:
	- There may be multiple paths to the same state
	- Improvement (due to search graph) depends on how costly it is to determine a node has already been visited
- ### Graph Search Algorithm
  
  ```
  function GRAPH-SEARCH(problem) returns a solution or failure
  • Initialize the frontier using the initial state of problem
  • Initialize the explored set (closed) to empty
  • Loop
  1. if the frontier is empty then return failure
  2. choose a leaf node and remove it from the frontier
  3. if the node contains a goal state then return the corresponding solution
  4. add the node to the explored set
  5. expand the chosen node, merging the resulting nodes with the frontier or the explored set
  end
  ```
- ### Basic Search Algorithm: Key Issues
- Return a path or a node?
- **Unboundedness:**
	- Tree search: because of loops
	- Graph/tree search: because the state space is infinite
- **Tree search: Repeated states**
	- Failure to detect repeated states can increase the complexity of a problem
- **How are the nodes ordered?** → Search strategy
	- Is the graph weighted or unweighted?
	- How much is known about the "quality" of intermediate states?
	- Is the aim to find a minimal cost path or any path asap?
- ### Dealing with Repeated States
  
  **3 ways to deal with repeated states** (ordered by cost and effectiveness):
  
  1. Do not return to the state you just came from
   → don't generate successors with same state as a node's parent
  
  2. Do not create paths with cycles in them
   → don't generate successors with same state as any ancestor
  
  3. Do not generate any state that was ever generated before
   → Use hashset to check if state has been visited
- ### Implementation of the Graphsearch Algorithm
  
  ```
  1. Create a search graph G consisting only of the start node s
  2. OPEN ← s
  3. CLOSED ← ∅
  4. Loop
   1. If OPEN = ∅ Then exit with failure
   2. n ← first node in OPEN
      Remove n from OPEN, put it in CLOSED
   3. If n = goal-node Then exit successfully with the solution obtained by 
      tracing a path along the pointers from n to s in G
   4. Expand node n, generating a set M of its children that are not
      ancestors of n. Put these members of M as children of n in G.
   5. Establish a pointer to n from those members of M that were not 
      already in G. Add these members of M to OPEN. For each member 
      of M already in G, decide whether or not to redirect its pointer to n.
   6. Reorder OPEN (according to an arbitrary scheme or merit)
  End
  ```
  
  ---
- ## Tree and Graph Search Strategies
- ### Search Strategies
- A **search strategy** is defined by picking the order of node expansion
- Strategies are evaluated along several dimensions:
	- **completeness**: does it always find a solution if one exists?
	- **time complexity**: maximum number of nodes generated
	- **space complexity**: maximum number of nodes in memory
	- **optimality**: does it always find a least-cost solution?
	  
	  **Time and space complexity are measured in terms of:**
- $b$: maximum branching factor of the search tree
- $d$: depth of the least-cost solution
- $m$: maximum depth of any path in the search space (may be ∞)
- ### O Notation
- $n$ measures the size of the input
- $f(n)$ is a function characterizing the worst-case complexity of an algorithm
- $O(f(n))$ is the set of all functions (eventually, asymptotically) bounded from above by some positive multiple $k$ of $f(n)$
  
  **Example:**
  
  Let $n$ be the number of items to be sorted, then:
- Bubble sort has worst case $k_1n^2$; i.e., $O(n^2)$
- Heap sort has worst case $k_2n \log n$; i.e., $O(n \log n)$
  
  ---
- ## Uninformed Search Strategies
  
  Uninformed search strategies use only the information available in the problem definition:
- **Breadth-first search (BFS)**
- **Uniform-cost search (UCS)**
- **Depth-first search (DFS)**
- **Depth-limited search (DLS)**
- **Iterative deepening search (IDS)**
- ### Breadth-first Search (BFS)
- Expand shallowest unexpanded node
- **Implementation: managing the frontier**
	- QUEUEING-FN: FIFO – put successors at end of queue
- ### Properties of Breadth-First Search
  
  **Complete?** Yes (if $b$ is finite)
  
  **Time?** $1 + b + b^2 + \ldots + b^d = \frac{b^{d+1}-1}{b-1} \to O(b^d)$
  
  **Space?** $O(b^d)$ (keeps every node in memory)
  
  **Optimal?** Yes (if all actions have the same cost)
  
  > Space is the bigger problem
  > Only tree/graphsearch algorithm that can stop when the goal node is reached
- ### Uniform-Cost Search Algorithm
  
  ```
  function UNIFORM-COST-SEARCH(problem) returns a solution or failure
  • Initialize the frontier using the initial state of problem
  • Loop
  1. if the frontier is empty then return failure
  2. choose the lowest-cost node in the frontier and remove it from the frontier
  3. if the node contains a goal state then return the corresponding solution
  4. expand the chosen node
     a. if the resulting nodes are not in the frontier then add them to the frontier
     b. else if the resulting nodes are in the frontier with higher path cost 
        then replace them with the new nodes
  end
  ```
- ### Properties of Uniform-cost Search
- Almost equivalent to BFS if step costs all equal
- **Complete?** Yes, if step cost ≥ ε
- **Time?** $O(b^{1+\lfloor C^*/\epsilon \rfloor})$ where $C^*$ is the cost of the optimal solution
- **Space?** $O(b^{1+\lfloor C^*/\epsilon \rfloor})$
- **Optimal?** Yes – nodes expanded in increasing order of $g(n)$ = cost of path to node $n$
  
  **Questions:**
  1. When all step costs are the same, UCS does more work than BFS. Why?
  2. When UCS selects a node for expansion, the optimal path to that node has been found.
- ### Depth-first Search (DFS)
- Expand the deepest unexpanded node
- **Implementation: managing the frontier**
	- QUEUEING-FN: LIFO – insert successors in front of queue
- ### Properties of Depth-first Search
  
  **Complete?**
- Infinite-state spaces: No
- Finite-state spaces: Yes, if we check for ancestors
  
  **Time?** $O(b^m)$, terrible if $m$ is much larger than $d$
  
  **Space?** $O(bm)$, i.e., linear space
  
  **Optimal?** No
  
  > **Question**: When all step costs are the same, will DFS find the optimal path?
- ### Depth-limited Search
- Depth-first search with depth limit $L$
	- nodes at depth $L$ have no successors
	- returns cut-off if no solution is found
	  
	  **Complete?** No if $d > L$
	  
	  **Time?** $1 + b + b^2 + \ldots + b^L = \frac{b^{L+1}-1}{b-1} \to O(b^L)$
	  
	  **Space?** $O(bL)$
	  
	  **Optimal?** No
	  
	  > **Question**: When all step costs are the same, will DLS find the optimal path?
- ### Iterative Deepening DF Search
  
  ```
  function ITERATIVE-DEEPENING-DF-SEARCH(problem) returns a solution or failure
  • Initialize the frontier using the initial state of problem
  • For depth ← 0 to ∞
  • result ← DEPTH-LIMITED-SEARCH(problem, depth)
  • if result ≠ cut-off then return result
  • end
  ```
  (indicates failure)
- ### Iterative Deepening Search – Generated Nodes
  
  **Number of nodes generated in a depth-limited search to depth $d$ with branching factor $b$:**
  
  $$N_{DLS} = 1 + b + b^2 + \ldots + b^d = \frac{b^{d+1}-1}{b-1} \to O(b^d)$$
  
  **Number of nodes generated in an iterative deepening search to depth $d$ with branching factor $b$:**
  
  $$N_{IDS} = d \cdot 1 + (d-1)b + \ldots + 3b^{d-3} + 2b^{d-2} + 1b^{d-1} + 1b^d \to O(b^d)$$
  
  **Example:** For $b = 10$, $d = 6$:
- $N_{DLS} = 1 + 10 + 100 + 1,000 + 10,000 + 100,000 = 111,111$
- $N_{IDS} = 6 + 50 + 400 + 3,000 + 20,000 + 100,000 = 123,456$
- Overhead = $\frac{123,456 - 111,111}{111,111} = 11\%$
- ### Properties of Iterative Deepening Search
  
  **Complete?** Yes
  
  **Time?** $d \cdot 1 + (d-1)b + \ldots + 3b^{d-3} + 2b^{d-2} + 1b^{d-1} + 1b^d \to O(b^d)$
  
  **Space?** $O(bd)$
  
  **Optimal?** Yes, if step costs are identical
  
  ---
  
  This completes the transcription of the first 3 weeks of lectures. Would you like me to continue with the remaining weeks?
- # Week 4-5: Informed Search Strategies and Knowledge Representation
- ## Informed Search Strategies: Best-first Search
- ### Heuristic (Informed) Graphsearch Procedures
- Use **Heuristic Information** (domain dependent information) to help reduce the search
	- **Evaluation function** – a real valued function used to compute the "promise" of a node
- ### Heuristic Graphsearch: Definitions (I)
- $k(n_i, n_j)$ – actual cost of minimal cost path between $n_i$ and $n_j$
- $h^*(n) = \min\{k(n, t_i)\}$ – minimum of all the $k(n, t_i)$ over the entire set of goal nodes $\{t_i\}$
- $g^*(n) = k(s, n)$ – minimum cost from the start node $s$ to $n$
- $f^*(n) = g^*(n) + h^*(n)$ – cost of an optimal path constrained to go through $n$
- $f^*(s) = h^*(s)$ – cost of an unconstrained optimal path from $s$ to goal
- ### Heuristic Graphsearch: Definitions (II)
- $f(n)$ – estimate of the minimal cost path constrained to go through node $n$
- $g(n)$ – estimate of $g^*(n)$ ($g(n) \geq 0$)
	- **Usual choice**: Cost of the path in the search tree/graph from $s$ to $n$ → $g(n) \geq g^*(n)$
- $h(n)$ – **heuristic function**
	- Estimate of $h^*(n)$ ($h(n) \geq 0$)
- ### Greedy Best-First Search
- Expands the node that is closest to the goal among the current options
	- $f(n) = h(n)$
	- **Example**: $h_{SLD}(n)$ = Straight-Line Distance to the goal
- ### Properties of Greedy Best-First Search
  
  **Complete?**
- Infinite-state spaces: No
- Finite-state spaces: Yes, if we check for ancestors
  
  **Time?** $O(b^m)$
  
  **Space?** $O(b^m)$
  
  **Optimal?** No
- ### Algorithm A
- Graphsearch using the evaluation function $f(n) = g(n) + h(n)$
- $g(n) \geq g^*(n)$; $h(n) \geq 0$
- Expands next the node in the frontier with the smallest value of $f(n)$
- ### Algorithm A* Example – Shortest Path
  
  **ROAD DISTANCES:**
  
  | | A | B | C | D | E | F |
  |---|---|---|---|---|---|---|
  | A | | 10 | 7 | | | |
  | B | | | 11 | 9 | | |
  | C | | | | 11 | 15 | |
  | D | | | | | 5 | 16 |
  | E | | | | | | 11 |
  
  **AIR DISTANCES:**
  
  | | A | B | C | D | E | F |
  |---|---|---|---|---|---|---|
  | A | | 4 | 3 | 8 | 12 | 20 |
  | B | | | 6 | 5 | 9 | 18 |
  | C | | | | 7 | 10 | 19 |
  | D | | | | | 5 | 15 |
  | E | | | | | | 10 |
- ### Algorithm A + Admissibility and Consistency
  
  **Admissibility of $h$:**
  
  If $\forall n: h(n) \leq h^*(n)$
  
  Then A* is guaranteed to find the optimal solution (if it exists)
  
  **Monotonicity (Consistency) of $h$:**
  
  If $\forall n: h(n) \leq c(n, m) + h(m)$
  
  where $m$ is a child of $n$
  
  Then A* has found the optimal path to any node it selects for expansion
  
  **Optimality of A*:**
- General graphsearch (Nilsson and classnotes) is optimal and terminates (if there is a solution) if $h(n)$ is admissible
- Restricted graphsearch (Russell & Norvig) is optimal and terminates if $h(n)$ is consistent
- ### h(n) – Admissibility
  
  **If** $\forall n: h(n) \leq h^*(n)$
  
  **Then** A* is guaranteed to find the optimal solution (if it exists)
- ### h(n) – Monotonicity
  
  **If** $\forall n: h(n) \leq c(n, m) + h(m)$, where $m$ is a child of $n$
  
  **Then** A* has found the optimal path to any node it selects for expansion
- ### Properties of A and A*
  
  | | A | A* |
  |---|---|---|
  | Complete? | Yes | Yes |
  | Time? | $O(b^d)$ | $O(b^\Delta)$, where $\Delta \propto \max\|h-h^*\|$ |
  | Space? | $O(b^d)$ | $O(b^\Delta)$ |
  | Optimal? | No | Yes |
- ### Admissible Heuristics: 8 Puzzle
- $h_1(n)$ = number of misplaced tiles
- $h_2(n)$ = total Manhattan distance (# of squares from desired location of each tile)
  
  **Questions:**
- $h_1(S)$ = ?
- $h_2(S)$ = ?
- ### Relaxed Problems
- A problem with fewer restrictions on the actions is called a **relaxed problem**
- The cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem
  
  **Examples:**
- If the rules of the 8-puzzle are relaxed so that a tile can move anywhere, then $h_1(n)$ gives the shortest solution
- If the rules are relaxed so that a tile can move to any adjacent square, then $h_2(n)$ gives the shortest solution
- ### Dominance
- Given two admissible heuristics $h_1$ and $h_2$, if $h_2(n) \geq h_1(n)$ for all $n$ then $h_2$ **dominates** $h_1$
  → $h_2$ is better for search
- If we have several admissible heuristics $h_1, h_2, \ldots, h_n$, none of which dominates, we can take the maximum:
  
  $$h(i) = \max\{h_1(i), h_2(i), \ldots, h_n(i)\}$$
- ### Measuring Performance
  
  Performance is often measured by **effective branching factor (EBF)** $b^*$
- If $N$ nodes are generated, this is the branching factor that a uniform tree of depth $d$ would have to have to contain $N+1$ nodes, i.e.,
  
  $$N + 1 = 1 + b^* + (b^*)^2 + \ldots + (b^*)^d = \frac{(b^*)^{d+1} - 1}{b^* - 1}$$
  
  Approximating with $b^* \approx 2$:
  
  $$N + 2 \approx 2^{d+1}$$
  
  **Example:** $N=52$, $d=5$ → $b^* \approx 1.9$
  
  > Experimental measurements of $b^*$ on a small set of problems can provide an idea of a heuristic's usefulness
  > - A good heuristic yields $b^* \approx 1$
- ### Summary: Tree- and Graph-Search
- When an agent is not clear on which immediate action is best, it can consider possible sequences of actions: **search**
- Before solutions can be found, the agent must formulate a goal and a problem, which consist of:
	- the initial state; a set of operators; a set of constraints; a goal test function; a path cost function
- A single general search algorithm can be used to solve any search problem
- Different search strategies yield different algorithms, which are judged on the basis of:
	- completeness; optimality; time complexity; space complexity
	  
	  ---
- ## Irrevocable Search Algorithms
- ### Local Search Algorithms
- In many optimization problems, the goal state is the solution
- State space = set of "complete" configurations
- Find configuration satisfying constraints, e.g., n-queens problem
- In such cases, we can use **local search algorithms**
	- keep a single "current" state, try to improve it
- ### Example: n-Queens Problem
- Put $n$ queens on an $n \times n$ board with no two queens on the same row, column or diagonal
- ### Hill Climbing Algorithm
  
  ```
  Procedure Hill Climbing(current-state)
  1. If current-state = goal-state Then return it
  2. Else until a solution is found or no more operators can be applied do
   a. Select an operator that has not been applied yet to current-state 
      and apply it to generate new-state
   b. Evaluate new-state:
      i. If new-state = goal-state Then return it and quit
      ii. Elseif new-state is better than current-state Then
          current-state ← new-state
  ```
  
  **Steepest ascent hill-climbing**: select the best operator
- ### Hill Climbing – Example 8 Puzzle
  
  **$f$ = - { number of tiles out of place }**
  
  ```
  2 _ 3         1 2 3         1 2 3
  1 8 4    →    8 _ 4    →    8 _ 4
  7 6 5         7 6 5         7 6 5
  f= -2         f=-1          f= 0
  ```
- ### Hill-climbing Search: Problem
  
  Depending on initial state, can get stuck in local maxima
- ### Local Beam Search
- Keep track of $k$ states rather than just one
- Start with $k$ randomly generated states
- At each iteration, all the successors of all $k$ states are generated
	- If any one is a goal state, stop
	- Else select the $k$ best successors from the complete list and repeat
- ### Simulated Annealing
- Based on the physical process of annealing
- **Idea**: escape local maxima/minima by allowing some "bad" moves, but gradually decrease their frequency
- **Temperature (T)** – the temperature at which the annealing takes place
- **Annealing schedule** – the rate at which the temperature is lowered
- ### Simulated Annealing Algorithm
  
  ```
  Procedure Simulated Annealing(current-state)
  1. If current-state = goal-state Then return it and quit
  2. BestSoFar ← current-state
  3. Initialize T according to the annealing schedule
  4. Until no more operators can be applied do
   a. Select an operator that has not been applied yet to current-state 
      and apply it to generate new-state
   b. Evaluate new-state. Compute:
      ΔE = Value(current-state) - Value(new-state)
      i. If new-state = goal-state Then return it and quit
      ii. Elseif ΔE<0 (new-state is better than current-state) Then
          current-state ← new-state
          If new-state is better than BestSoFar Then BestSoFar ← new-state
      iii. Else with probability Pr=e^(-ΔE/T) current-state ← new-state
   c. Revise T according to the annealing schedule
   d. If T=0 Then return BestSoFar
  ```
  
  (Maximization problem)
- ### Properties of Simulated Annealing Search
- One can prove: If $T$ decreases slowly enough, then simulated annealing search will find a global optimum with probability approaching 1
- It is widely used in VLSI layout and airline scheduling
- ### Genetic Algorithms
- Start with a population of $k$ randomly generated states
- A **state (chromosome)** is represented as a string over a finite alphabet of genes (often a string of 0s and 1s)
- A **successor state** is generated by combining two parent states
- **Evaluation function (fitness function)**:
	- Higher values for better states
- Produce the next generation of states by **selection, crossover and mutation**
- ### GAs: Example 8-Queens Problem (I)
  
  **Representation:**
- **Gene**: row # (between 1 and 8) of the queen that is in column $i$
- **Chromosome**: 1 gene per column (8 genes per chromosome)
  
  **Fitness function:** number of non-attacking pairs of queens (min = 0, max = $8 \times 7/2 = 28$)
  
  **Probability of selection:**
- $\frac{24}{24+23+20+11} = 31\%$
- $\frac{23}{24+23+20+11} = 29\%$
- ### Search Algorithms – A Perspective
  
  **All search strategies can be viewed as:**
- Backtrack algorithms
- Graphsearch
	- **Uninformed**: BFS, UCS, DFS, DLS, IDS
	- **Informed**: Greedy BestFS, A, A*
- **Irrevocable**: Hill climbing, Simulated annealing, Genetic algorithms
  
  **Informedness in Graphsearch** depends on $g$ and $h$:
- A: $f(n) = g(n) + h(n)$ ($g(n) \geq g^*(n)$, $h(n) \geq 0$)
- A*: ($g(n) \geq g^*(n)$, $h(n) \leq h^*(n)$)
  
  **Uninformed Graphsearch:**
- BFS ∈ A* when $g(n)=\text{depth}$ and $h(n)=0$
- UCS ∈ A* with $g(n)\geq 0$ and $h(n)=0$
- DFS, DLS, IDS ∈ Graphsearch, DFS, DLS, IDS ∈ A
  
  **Informed Graphsearch:**
- Greedy best-first search with $g(n)=0$ and $h(n)\geq 0$ ∈ A
  
  ---
- ## Adversarial Search Algorithms
- ### Searching Game Trees
  
  **Two person, perfect information games**
  
  **Conventions:**
- Players are **MAX** and **MIN**
	- A position favourable to MAX has a value > 0 (winning is often ∞)
	- A position favourable to MIN has a value < 0 (winning is often −∞)
- **Goal**: find a winning strategy for MAX
	- For all nodes representing a game situation where it is MIN's move next, show that MAX can win from every position to which MIN might move
	- For all nodes representing a game situation where it is MAX's move next, show that MAX can win from just one position to which MAX might move
- ### Games versus Search Problems
- **Unpredictable opponent** → must specify a move for every possible opponent reply
- **Time limits**: not all games can be searched to the end → find a good first move
- ### Minimax Ideas
- If MAX were to choose among tip nodes, s/he would take the node with the largest value
- If MIN were to choose among tip nodes, s/he would take the node with the smallest value
- **Choose move to the position with highest minimax value**: best achievable payoff against best play
- ### Minimax Example: Tic-Tac-Toe
  
  **Evaluation function:**
  { # of rows, columns, diagonals available to MAX – # of rows, columns, diagonals available to MIN }
- ### Properties of Minimax
  
  Minimax implements Complete Depth First Exploration
- All paths are explored to depth $m$
  
  **Complete?** Yes (if tree is finite)
  
  **Optimal?** Yes (against an optimal opponent)
  
  **Time complexity?** $O(b^m)$
  
  **Space complexity?** $O(bm)$ (depth-first exploration)
  
  For chess, $b \approx 35$, $m \approx 100$ for "reasonable" games → exact solution completely infeasible
- ### Resource Limits
  
  Suppose we have 100 secs per move, and we explore $10^4$ nodes/sec → $10^6$ nodes per move
  
  **Standard approach:**
- **Cutoff test** – depth limit (perhaps add quiescence search)
- **Evaluation function** – estimates the desirability of a position
	- E.g., for chess typically a linear weighted sum of features
	  
	  $$\text{Eval}(s) = w_1 f_1(s) + w_2 f_2(s) + \ldots + w_n f_n(s)$$
	  
	  where $w_1 = 9$ and $f_1(s) =$ (# of white queens) – (# of black queens)
- **Forward pruning**
	- beam search that looks only at n-best moves
- ### Definitions: α and β Values
- **α-value of a MAX node** – current largest final backed-up value of its successors
	- α-value is the lower bound for a MAX backed-up value
- **β-value of a MIN node** – current smallest final backed-up value of its successors
	- β-value is the upper bound for a MIN backed-up value
- ### α-β Procedure
  
  **Rules for discontinuing the search:**
- **α cut-off**: search can be discontinued below any MIN node having a β-value ≤ α-value of any of its MAX node ancestors
	- The final backed-up value of this MIN node is set to its β-value
- **β cut-off**: search can be discontinued below any MAX node having an α-value ≥ β-value of any of its MIN node ancestors
	- The final backed-up value of this MAX node is set to its α-value
- ### Termination Condition
- All the successors of the start node are given final backed-up values
- The best first move is that which creates the successor with the highest backed-up value
- ### The α-β Algorithm
  
  **α** = the value of the best (i.e., highest-value) choice we have found so far at any choice point along the path for MAX
  
  **β** = the value of the best (i.e., lowest-value) choice we have found so far at any choice point along the path for MIN
- ### Move Ordering
- The effectiveness of the αβ algorithm depends on the order in which states are examined
- With perfect ordering, time complexity = $O(b^{m/2})$
  → depth of search can be doubled
- Adding dynamic ordering schemes brings us close to the theoretical limit
- ### Deterministic Games in Practice
- **Checkers**: Chinook defeated the world champion in an abbreviated game in 1990. It uses αβ search combined with a pre-computed database defining perfect play for 39 trillion endgame positions.
- **Chess**: Deep Blue defeated human world champion Garry Kasparov in a six-game match in 1997. Deep Blue searches 30 billion positions per move (200 million per second), normally searching to depth 14, and extending the search up to depth 40 for promising options. Heuristics reduce the EBF to about 3.
- **Othello**: In 1997, a computer defeated the world champion 6-0. Humans are no match for computers.
- **Go**: $b > 361$, which is too large for αβ. In 2016, AlphaGo, which uses Deep Learning, beat the world champion 4-1.
- ### Summary: Adversarial Search
  
  Games illustrate important points about AI:
- Perfection is unattainable → must approximate
- Force us to think about what to think about, e.g., nodes to keep/discard
- ### Reading
  
  Russell, S. and Norvig, P. (2010), *Artificial Intelligence – A Modern Approach* (3nd ed), Prentice Hall
- Chapter 7, Sections 7.1, 7.3 (only backtrack algorithm)
- Chapter 3 (excluding 3.5.3, 3.5.4, 3.6.3, 3.6.4)
- Chapter 4, Section 4.1
- Chapter 5, Sections 5.1-5.4
  
  ---
- # Week 6-7: Knowledge Representation
- ## Lecture Topic 4 (LN4): Knowledge Representation
- ### Knowledge Representation: Learning Objectives
  
  **Propositional logic:**
- Knowledge-based agents
- Logic in general – models and entailment
- Propositional (Boolean) logic
- Equivalence, validity, satisfiability
  
  **First-order logic:**
- First-order logic
- Inference rules and theorem proving
	- forward chaining
	- backward chaining
	- resolution refutation systems
		- substitution
		- unification
		- resolution
- ### Assumptions About the Environment
- Observable
- Known
- Single/multi agent
- Deterministic
- Sequential/episodic
- Static
- Discrete
- ### Knowledge Representation
  
  **How do we represent facts about the world?**
  
  **How do we reason about these facts?**
  
  **Some widely accepted formal calculi:**
- Propositional logic
- First-order logic
- Probability calculus
- ### Knowledge Bases
- **Knowledge base** = set of sentences in a formal language
- **Declarative approach** to building an agent:
	- Tell it what it needs to know
	- Ask it a question – answers follow by inference from the KB
- Agents may be viewed:
	- at the **knowledge level** – what they know
	- at the **implementation level** – data structures in the KB and algorithms that manipulate them
- ### A Simple Knowledge-Based Agent
  
  The agent must be able to:
- Represent states, actions, etc
- Incorporate new percepts
- Update internal representations of the world
- Deduce hidden properties of the world
- Deduce appropriate actions
- ### Logic in General
- **Logics** are formal languages for representing information such that conclusions can be drawn
- **Syntax** defines the sentences in the language
- **Semantics** define the meaning of sentences
	- the truth of a sentence in each possible world
	  
	  **E.g., the language of arithmetic:**
- $x+2 \geq y$ is a sentence; $x^2+y > \{\}$ is not a sentence
- $x+2 \geq y$ is true in all the worlds where the number $x+2$ is no less than the number $y$
	- $x+2 \geq y$ is true in a world where $x = 7$, $y = 1$
	- $x+2 \geq y$ is false in a world where $x = 0$, $y = 6$
- ### Logical Entailment
- **Entailment** means that one thing follows logically from another:
  
  $$KB \models \alpha$$
  
  knowledge base KB entails sentence α **iff** α is true in all worlds where KB is true
  
  **E.g.:**
- the KB containing "the Giants won" and "the Reds won" entails "the Giants won or the Reds won"
- $x+y = 4$ entails $4 = x+y$
  
  > Entailment is a relationship between sentences (i.e., syntax) that is based on semantics
- ### Models
- **Models** are formally structured worlds with respect to which truth can be evaluated
- If a sentence α is true in a model $m$, we say that
	- $m$ is a model of α, or
	- $m$ satisfies α
- $M(\alpha)$ = the set of all models of α
	- $KB \models \alpha$ iff $M(KB) \subseteq M(\alpha)$
	  
	  **E.g.:** KB = Giants won & Reds won; α = Giants won
- ### Inference
- $KB \vdash_i \alpha$ means that sentence α can be derived from KB by procedure $i$
- **Soundness**: procedure $i$ is sound if whenever $KB \vdash_i \alpha$, it is also true that $KB \models \alpha$
- **Completeness**: procedure $i$ is complete if whenever $KB \models \alpha$, it is also true that $KB \vdash_i \alpha$
- **First-order logic (FOL)** is expressive enough to say almost anything of interest, and there exists a sound and complete inference procedure for it
- ### Grounding
  
  About the connection between logical reasoning processes and real environments
  
  **How do we know that KB is true in the real world?**
- By creating a connection using the agent's sensors, i.e., the meaning and truth of percept sentences are defined by the processes of sensing and sentence construction
  
  **Where do we get the rest of an agent's knowledge?**
- By learning (generalizing) from experience
  
  ---
- ## Propositional Logic
- ### Propositional Logic: Some Definitions
- **Literal**: a proposition or its negation
	- E.g., $P$, $\neg P$
- **Clause**: a disjunction of literals
	- E.g., $\neg P \lor Q \lor A$
- ### Propositional Logic: Syntax
- Propositional logic is the "simplest" logic
- The proposition symbols $P_1, P_2, \ldots$ are sentences
	- **Negation**: If $S$ is a sentence, $\neg S$ is a sentence
	- **Conjunction**: If $S_1$ and $S_2$ are sentences, $S_1 \land S_2$ is a sentence
	- **Disjunction**: If $S_1$ and $S_2$ are sentences, $S_1 \lor S_2$ is a sentence
	- **Implication**: If $S_1$ and $S_2$ are sentences, $S_1 \Rightarrow S_2$ is a sentence
	- **Biconditional**: If $S_1$ and $S_2$ are sentences, $S_1 \Leftrightarrow S_2$ is a sentence
- ### Propositional Logic: Semantics
- Each model specifies true/false for each proposition
	- E.g., $S_1$ $S_2$ $S_3$: false true false
	  
	  **Rules for evaluating truth with respect to a model $m$:**
- $\neg S$ is true iff $S$ is false
- $S_1 \land S_2$ is true iff $S_1$ is true and $S_2$ is true
- $S_1 \lor S_2$ is true iff $S_1$ is true or $S_2$ is true
- $S_1 \Rightarrow S_2 \equiv \neg S_1 \lor S_2$ is true iff $S_1$ is false or $S_2$ is true
- $S_1 \Leftrightarrow S_2$ is true iff $S_1 \Rightarrow S_2$ is true and $S_2 \Rightarrow S_1$ is true
  
  **Simple recursive process evaluates any sentence:**
  
  E.g., $\neg S_1 \land (S_2 \lor S_3) = \text{true} \land (\text{true} \lor \text{false}) = \text{true} \land \text{true} = \text{true}$
- ### Truth Tables for Connectives
  
  [Standard truth tables for $\neg$, $\land$, $\lor$, $\Rightarrow$, $\Leftrightarrow$]
- ### Logical Equivalence
  
  Two sentences are **logically equivalent** iff they are both true in the same models: $\alpha \equiv \beta$ iff $\alpha \models \beta$ and $\beta \models \alpha$
- ### Validity and Satisfiability
- A sentence is **valid** if it is true in all models
	- E.g., True, $A \lor \neg A$, $A \Rightarrow A$
- **Validity is connected to inference** via the Deduction Theorem:
	- $\alpha \models \beta$ iff $\alpha \Rightarrow \beta$ is valid
- A sentence is **satisfiable** if it is true in some model
	- E.g., $A \lor B$, $C$
- A sentence is **unsatisfiable** if it is true in no model
	- E.g., $A \land \neg A$
	  
	  **Satisfiability and validity are connected:**
- $\alpha$ is valid iff $\neg \alpha$ is unsatisfiable
- $\alpha$ is satisfiable iff $\neg \alpha$ is not valid
- $\alpha \models \beta$ iff $\alpha \land \neg \beta$ is unsatisfiable
- ### Validity: Example Proof
  
  **Prove that** $(A \land (\neg A \lor B)) \Rightarrow B$ **is valid**
  
  $(A \land (\neg A \lor B)) \Rightarrow B$
  $\equiv ((A \land \neg A) \lor (A \land B)) \Rightarrow B$
  $\equiv (A \land B) \Rightarrow B$
  $\equiv \neg(A \land B) \lor B$
  $\equiv \neg A \lor \neg B \lor B$
  $\equiv \text{True}$
  
  Or: $(A \land (A \Rightarrow B)) \Rightarrow B$
- ### Validity and Satisfiability: Example Proofs
  
  **Prove that** $\alpha$ **is valid iff** $\neg \alpha$ **is unsatisfiable**
- If $\alpha$ is valid, it is true in all models → there does not exist a model for which $\neg \alpha$ is true → $\neg \alpha$ is unsatisfiable
- If $\neg \alpha$ is unsatisfiable → there does not exist a model for which $\neg \alpha$ is true → $\alpha$ is true in all models → $\alpha$ is valid
  
  **Prove that** $\alpha$ **is satisfiable iff** $\neg \alpha$ **is not valid**
- If $\alpha$ is satisfiable, it is true in some models → $\neg \alpha$ is not true in these models → $\neg \alpha$ is not valid
- If $\neg \alpha$ is not valid → there exist some models for which $\neg \alpha$ is false → $\alpha$ is true in these models → $\alpha$ is satisfiable
- ### Proof Methods
  
  **Model checking:**
  Enumerates all possible models to check that a sentence α is true in all models where KB is true
- Truth table enumeration ($2^n$, where $n$ is the number of symbols)
- Backtracking (recursive enumeration of all models) with logic-related heuristics
  
  **Application of inference rules:**
  Sound generation of new sentences from old
- Proof = a sequence of inference rule applications
	- use inference rules as operators in a standard search algorithm
	- typically require transformation of sentences into a normal form
- ### Common Rules of Inference
  
  | Parent clauses | Resolvent | Name |
  |---|---|---|
  | $P$ and $\neg P \lor Q$ | $Q$ | Modus Ponens |
  | $\neg Q$ and $\neg P \lor Q$ | $\neg P$ | Modus Tollens |
  | $P$ and $Q$ | $P$ or $Q$ | And Elimination |
  | $P \lor Q$ and $\neg P \lor Q$ | $Q$ | |
  | $P \lor Q$ and $\neg P \lor \neg Q$ | $Q \lor \neg Q$ or $P \lor \neg P$ | Tautology |
  | $P$ and $\neg P$ | NIL | |
  | $\neg P \lor Q$ and $\neg Q \lor R$ | $\neg P \lor R$ | Chaining |
  | $P \lor Q$ and $\neg P \lor R$ | $Q \lor R$ | |
- ### Resolution: Example
  
  1. Eat → HaveLessMoney
  2. ¬Eat → Hungry
  
  $\neg$HaveLessMoney → Hungry
  
  | Eat | HLM | Hungry | ¬Eat ∨ HLM | Eat ∨ Hungry | HLM ∨ Hungry |
  |---|---|---|---|---|---|
  | F | F | F | T | F | F |
  | F | F | T | T | T | T |
  | F | T | F | T | F | T |
  | F | T | T | T | T | T |
  | T | F | F | F | T | F |
  | T | F | T | F | T | T |
  | T | T | F | T | T | T |
  | T | T | T | T | T | T |
  
  **Resolvent:** HaveLessMoney ∨ Hungry
- ### Proof as Search
- **Initial state**: initial KB
- **Actions**: the inference rules applied to all the sentences that match the LHS of the rule
- **Result**: add the sentence on the RHS of a rule to the KB
- **Goal**: a state where the KB contains the sentence we are trying to prove
  
  > **Monotonicity**: the set of entailed sentences can only increase as information is added to the KB
- ### Resolution
  
  **Resolution** – an inference rule applied to clauses that yields a complete inference algorithm when coupled with any complete search algorithm
  
  $$\frac{l_1 \lor \ldots \lor l_i \lor \ldots \lor l_k \quad m_1 \lor \ldots \lor m_j \lor \ldots \lor m_n}{l_1 \lor \ldots \lor l_{i-1} \lor l_{i+1} \lor \ldots \lor l_k \lor m_1 \lor \ldots \lor m_{j-1} \lor m_{j+1} \lor \ldots \lor m_n}$$
  
  where $l_i$ and $m_j$ are complementary literals ($l_i = \neg m_j$)
- the resolvent should contain only one copy of each literal
  
  > **Resolution is sound for propositional logic**
- ### Conversion to Conjunctive Normal Form
  
  Every sentence in propositional logic is logically equivalent to a conjunction of clauses
  
  **Converting a sentence to Conjunctive Normal Form (CNF):**
  
  1. Eliminate $\Leftrightarrow$, replacing $\alpha \Leftrightarrow \beta$ with $(\alpha \Rightarrow \beta) \land (\beta \Rightarrow \alpha)$
  2. Eliminate $\Rightarrow$, replacing $\alpha \Rightarrow \beta$ with $\neg \alpha \lor \beta$
  3. Move $\neg$ inwards by repeated application of the following equivalences:
	- double-negation: $\neg(\neg \alpha) \equiv \alpha$
	- de Morgan: $\neg(\alpha \land \beta) \equiv \neg \alpha \lor \neg \beta$
	- de Morgan: $\neg(\alpha \lor \beta) \equiv \neg \alpha \land \neg \beta$
	  4. Apply the distributivity law ($\land$ over $\lor$) and flatten
- ### Conversion to CNF: Example
  
  $A \Leftrightarrow (B \lor C)$
  
  1. Eliminate $\Leftrightarrow$: $(A \Rightarrow (B \lor C)) \land ((B \lor C) \Rightarrow A)$
  2. Eliminate $\Rightarrow$: $(\neg A \lor (B \lor C)) \land (\neg(B \lor C) \lor A)$
  3. Move $\neg$ inwards: $(\neg A \lor (B \lor C)) \land ((\neg B \land \neg C) \lor A)$
   $(\neg A \lor B \lor C) \land ((\neg B \land \neg C) \lor A)$
  4. Apply the distributivity law:
   $(\neg A \lor B \lor C) \land (\neg B \lor A) \land (\neg C \lor A)$
- ### Resolution-Refutation Systems
  
  **Proof by refutation:**
  1. Negate the goal and add the negation to the set of clauses
  2. Apply resolution to the clauses in the set of clauses until a contradiction is reached
  
  **Answer extraction:**
  1. Build a tautology by appending the goal itself to the negation of the goal
  2. When the negated goal is contradicted, the answer resides in the goal
- ### Resolution-Refutation: Example
  
  **KB** = $(A \Leftrightarrow (B \lor C)) \land \neg A$
  $(\neg A \lor B \lor C) \land (\neg B \lor A) \land (\neg C \lor A) \land \neg A$
  
  **Prove**: $\alpha = \neg B$
  
  [Resolution proof tree showing derivation of NIL]
- ### Resolution-Refutation Algorithm
  
  Proof by contradiction, i.e., given a goal α, show that $KB \land \neg \alpha$ is unsatisfiable
- ### Soundness and Completeness
  
  **Resolution refutation is sound and complete**
- **Resolution Closure RC(S)** of a set of clauses $S$ is the set of all clauses derivable by repeated application of the resolution rule to the clauses in $S$ or their derivatives
	- $RC(S)$ is finite → Resolution always terminates
- **Ground resolution theorem**: if a set of clauses is unsatisfiable, then the resolution closure of those clauses contains the empty clause
  
  > Resolution-refutation is complete for propositional logic
- ### Resolution Refutation: Example
  
  **KB:**
- R1: $P \Rightarrow Q$
- R2: $L \land M \Rightarrow P$
- R3: $B \land L \Rightarrow M$
- R4: $A \land P \Rightarrow L$
- R5: $A \land B \Rightarrow L$
- $A$
- $B$
  
  **Prove Q**
  
  **KB (in CNF):**
- R1: $\neg P \lor Q$
- R2: $\neg L \lor \neg M \lor P$
- R3: $\neg B \lor \neg L \lor M$
- R4: $\neg A \lor \neg P \lor L$
- R5: $\neg A \lor \neg B \lor L$
- $A$
- $B$
  
  **$\neg Q$**
  
  [Resolution proof showing derivation of NIL]
- ### Horn Clauses and Definite Clauses
- **Definite clause** – a disjunction of literals of which exactly one is positive
	- E.g., $\neg A \lor \neg B \lor C$
	- Definite clauses can be written as implications $(A \land B) \Rightarrow C$
- **Horn clause** – a disjunction of literals of which at most one is positive
	- All definite clauses are Horn clauses
	- Clauses with no positive literals are goal clauses
	- Inference with Horn clauses can be done with forward or backward chaining
	- Deciding entailment with Horn clauses can be done in time that is linear on the size of the KB
- ### Backward Chaining
  
  **IDEA**: Work backwards from the query $q$:
- to prove $q$ by Backward Chaining, check if $q$ is known already, or prove by Backward Chaining all premises of some rule concluding $q$
  
  **Avoid loops**: check if new subgoal is already on the goal stack
  
  **Avoid repeated work**: check if a new subgoal
  1. has already been proved true, or
  2. has already failed
  
  > Backward chaining is sound and complete for Horn KBs
- ### Backward Chaining: Example
  
  **KB:**
- R1: $P \Rightarrow Q$
- R2: $L \land M \Rightarrow P$
- R3: $B \land L \Rightarrow M$
- R4: $A \land P \Rightarrow L$
- R5: $A \land B \Rightarrow L$
- $A$
- $B$
  
  **Prove Q**
  
  [Backward chaining proof tree]
- ### Forward Chaining Algorithm
  
  **IDEA**: Work forwards from the facts in KB
  
  > Forward chaining is sound and complete for Horn KBs
- ### Forward Chaining: Example
  
  **KB:**
- R1: $P \Rightarrow Q$
- R2: $L \land M \Rightarrow P$
- R3: $B \land L \Rightarrow M$
- R4: $A \land P \Rightarrow L$
- R5: $A \land B \Rightarrow L$
- $A$
- $B$
  
  **Prove Q**
  
  | Agenda | Count | | | | | Inferred |
  |---|---|---|---|---|---|---|
  | | R1 | R2 | R3 | R4 | R5 | |
  | A B | 1 | 2 | 2 | 2 | 2 | |
  | B | 1 | 2 | 2 | 1 | 1 | A |
  | | 1 | 2 | 1 | 1 | 0 | A B |
  | | 1 | 1 | 0 | 1 | 0 | A B L |
  | | 1 | 0 | 0 | 1 | 0 | A B L M |
  | | 0 | 0 | 0 | 0 | 0 | A B L M P |
- ### Forward versus Backward Chaining
  
  **Forward chaining:**
- data-driven, automatic, unconscious processing
- may do lots of work that is irrelevant to the goal
  
  **Backward chaining:**
- goal-driven, appropriate for problem-solving
- complexity can be much less than linear in size of KB
  
  > Both are sound and complete for Horn KBs
- ### Pros and Cons of Propositional Logic
  
  **Pros (✓):**
- declarative
- allows partial/disjunctive/negated information
- compositional:
	- meaning of $A \land B$ is derived from meaning of $A$ and of $B$
- Meaning is context-independent
  
  **Cons (✗):**
- Propositional logic has very limited expressive power
	- E.g., cannot say "all men are mortal"
	  
	  ---
- ## First Order Logic
- ### First-order Logic (FOL)
  
  **First-order logic assumes the world contains:**
- **Objects**: people, houses, numbers, colors, baseball games, wars, …
- **Relations**: red, round, prime, brother of, bigger than, part of, comes between, …
- **Functions**: father of, best friend, one more than, plus, …
  
  > FOL has increased expressive power
- ### First Order Logic – Syntax (I)
- **term** – constant, variable or function
- **atomic formula or atom** – predicate symbol and terms
	- Example: MARRIED(John, Mother(x))
		- Predicate symbol – MARRIED
		- Constant – John or A
		- Function – Mother or f
		- Variable – x
- **literal** – atomic formula or negation of an atomic formula
	- E.g., $\neg$MARRIED(John, Mother(x))
- **ground literal** – literal without variables
- ### First Order Logic – Syntax (II)
  
  **connectives:**
- disjunction – P(x) ∨ Q(y) ∨ W(x,y)
- conjunction – P(x) ∧ Q(y)
- implication – P(x) ⇒ W(x,y) º ¬P(x) ∨ W(x,y)
  
  **clause** – disjunction of literals
- E.g., MARRIED(John, Mother(x)) ∨ MARRIED(John, y)
  
  **conjunctive normal form** – a conjunction of a finite set of clauses
- E.g., (P₁(x) ∨ P₂(y)) ∧ P₃(x)
- ### Well Formed Formulas (wffs)
  
  Legitimate expressions in predicate calculus:
  
  1. Any conjunction of wffs is a wff
  2. Any disjunction of wffs is a wff
  3. If both the antecedent and the consequent are wffs, so is the implication
  4. The negation of a wff is also a wff
- ### First Order Logic: Quantification
  
  **Quantification:**
- **Universal (∀)** –
  $\forall x$ [ELEPHANT(x) ⇒ COLOUR(x, Gray)]
- **Existential (∃)** –
  $\exists x$ WRITE(x, Computer-Chess)
  
  **Additional wff rules:**
  5. Any expression obtained by quantifying a wff is a wff
- ### Which of These Expressions is a wff?
- $P(P(A))$
- $\neg f(P(A))$
- $P(A, g(A, B, A))$
- $\neg f(A)$
- $\forall x \exists y [P(x,y) \land Q(x,y)] \Rightarrow R(x)$
- ### FOL: Nesting of Quantifiers
- **Existential inside the scope of universal**
	- E.g., $\forall s \exists c$ Eats(s, c)
- **Universal inside the scope of existential**
	- E.g., $\exists c \forall s$ Eats(s, c)
- ### FOL: Some Equivalences
- $\neg(\exists x)P(x) \equiv (\forall x)[\neg P(x)]$
	- There does not exist an x such that P(x) is true ≡ For all x, P(x) is false
- $\neg(\forall x)P(x) \equiv (\exists x)[\neg P(x)]$
	- It is not true that for all x P(x) is true ≡ There exists an x, such that ¬P(x)
- $(\forall x)[P(x) \land Q(x)] \equiv (\forall x)P(x) \land (\forall y)Q(y)$
	- For all x, P(x) and Q(x) are true ≡ For all x, P(x) is true, and for all y Q(y) is true
- $(\exists x)[P(x) \lor Q(x)] \equiv (\exists x)P(x) \lor (\exists y)Q(y)$
	- There is an x, such that P(x) is true or Q(x) is true ≡ There is an x, such that P(x) is true, or there is a y, such that Q(y) is true
- ### Are These wffs Equivalent?
  
  $(\forall x)[P(x) \lor Q(x)]$ and $(\forall x)P(x) \lor (\forall y)Q(y)$
- ### Rules of Inference: Example
  
  **Modus Ponens:**
  $[P \text{ and } P \Rightarrow Q] \Rightarrow Q$
  
  **Modus Tollens:**
  $[\neg Q \text{ and } P \Rightarrow Q] \Rightarrow \neg P$
  
  **Universal Specialization:**
  $\forall x W(x) \Rightarrow W(A)$, where A is a constant
  
  **Example: Universal Specialization + Modus Ponens**
  
  1. $\forall x$ [DOG(x) ⇒ BARKS(x)]
  2. DOG(FIDO)
  
  From 1 and 2: BARKS(FIDO)
- ### General Inference: Resolution Refutation
  
  **Resolution:**
- Unification
	- Substitution
- Converting wffs to clauses
- ### Substitution
  
  **Substitution** is a set of ordered pairs:
  
  $$s = \{v_1|t_1, v_2|t_2, \ldots, v_n|t_n\}$$
  
  where $v_i|t_i$ means that term $t_i$ substitutes variable $v_i$ throughout
  
  **Example:**
  $P(x, y) \{x|A, y|B\} \Rightarrow P(A, B)$
  
  **Semantics**: all elements are applied simultaneously
  
  **Example:**
  $P(w, y, g(z), x) \{x|g(y), y|h(z), z|x\} \Rightarrow P(w, h(z), g(x), g(y))$
- ### Composition of Substitutions
  
  $s_i s_j$ – composition of two substitutions
- Apply $s_j$ to the terms of $s_i$
- Add any pairs of $s_j$ having variables not in $s_i$
  
  **Example:**
  
  $s_1 = \{z|g(x, y)\}$, $s_2 = \{x|A, y|B, w|C, z|D\}$
  
  $s_1 s_2 = \{z|g(x, y)\}\{x|A, y|B, w|C, z|D\} = \{z|g(A, B), x|A, y|B, w|C\}$
  
  $s_2 s_1 = \{x|A, y|B, w|C, z|D\}\{z|g(x, y)\} = \{x|A, y|B, w|C, z|D\}$
  
  **Properties of compositions of substitutions:**
- $L(s_1 s_2) = (Ls_1)s_2$
- Associative: $(s_1 s_2)s_3 = s_1(s_2 s_3)$
- NOT commutative: $s_1 s_2 \neq s_2 s_1$
- ### Unification
- A process that finds substitutions of terms for variables, such that two expressions are identical
- A set $\{E_i\}$ of expressions is **unifiable**, if there exists a substitution $s$ such that $E_1 s = E_2 s = \ldots$
	- In this case, $s$ is a **unifier** of $\{E_i\}$
- **mgu (most general unifier)** – the mgu $g$ of $\{E_i\}$ has the property that if $s$ is any unifier of $\{E_i\}$, yielding $\{E_i\}s$, then there exists a substitution $s'$ such that $\{E_i\}s = \{E_i\}gs'$
  
  **Example:**
  
  $s = \{x|A, y|B\}$ unifies $P(x, f(y))$ with $P(x, f(B))$
  
  but the mgu is $\{y|B\}$
- ### Algorithm Unify (List-Structured Expressions)
  
  ```
  Algorithm Unify(E1, E2)
  1. If either E1 or E2 is a variable or symbol, then interchange E1 and E2 if necessary, 
   so that E1 is a variable or symbol
   a. If E1 and E2 are identical then return { } // no substitution
   b. If E1 is a variable do
      i. If E1 occurs in E2 then return FAIL
      ii. return {E1|E2} // E2 may be a compound expression
   c. If E2 is a variable then return {E2|E1}
   d. return FAIL
  2. F1 ← the first element of E1, T1 ← rest of E1
  3. F2 ← the first element of E2, T2 ← rest of E2
  4. Z1 ← Unify(F1, F2)
  5. If Z1 = FAIL, then return FAIL
  6. G1 ← result of applying Z1 to T1; G2 ← result of applying Z1 to T2
  7. Z2 ← Unify(G1, G2)
  8. If Z2 = FAIL, then return FAIL
  9. return the composition of Z1 and Z2
  ```
  
  (predicate, function, negation, constant)
- ### Unification Example: Unify(P(y,g(y)), P(z,g(x)))
  
  [Detailed unification example walkthrough showing steps]
- ### Converting wffs into Clauses
  
  1. Eliminate implication symbols
  2. Reduce scopes of negation symbols
  3. Standardize variables (for each quantifier)
  4. Eliminate existential quantifiers (skolemize)
  5. Move all universal quantifiers to the front
  6. Put result in conjunctive normal form (CNF)
  7. Eliminate universal quantifiers
  8. Eliminate ∧ symbols
  9. Rename variables (standardize variables apart for each clause)
- ### Converting wffs into Clauses: Example 1
  
  $\forall x$ [CanRead(x) ⇒ Intelligent(x)]
  
  1. Eliminate ⇒: $\forall x$ [¬CanRead(x) ∨ Intelligent(x)]
  7. Eliminate ∀: ¬CanRead(x) ∨ Intelligent(x)
- ### Converting wffs into Clauses: Example 2
  
  $\forall x [\neg(\forall y)[P(x, y) \Rightarrow Q(x, y)]]$
  
  1. Eliminate ⇒: $\forall x [\neg(\forall y)[\neg P(x, y) \lor Q(x, y)]]$
  2. Reduce scope of ¬: $\forall x [\exists y \neg[\neg P(x, y) \lor Q(x, y)]]$
   $\forall x \exists y [P(x, y) \land \neg Q(x, y)]$
  4. Eliminate ∃: $\forall x [P(x, g(x)) \land \neg Q(x, g(x))]$
  7. Eliminate ∀: $P(x, g(x)) \land \neg Q(x, g(x))$
  8. Eliminate ∧ symbols: $\{P(x, g(x)), \neg Q(x, g(x))\}$
  9. Standardize variables apart: $\{P(x_1, g(x_1)), \neg Q(x_2, g(x_2))\}$
- ### General Resolution
  
  Let the prospective parent clauses be $\{L_j\}$ and $\{M_i\}$ (with variables standardized apart)
  
  Suppose that $\{l_j\}$ is a subset of $\{L_j\}$ and that $\{m_i\}$ is a subset of $\{M_i\}$ such that a most general unifier $s$ exists for the sets $\{l_j\}$ and $\{\neg m_i\}$
  
  The clauses $\{L_j\}$ and $\{M_i\}$ **resolve** and the new clause $\{\{L_j\} - \{l_j\}\}s \cup \{\{M_i\} - \{m_i\}\}s$ is a **resolvent** of the two clauses
- ### General Resolution: Example
  
  1. Everyone who can read is literate
  2. Whoever goes to school can read
  
  $\forall x$ [CANREAD(x) ⇒ LITERATE(x)]
  $\forall x$ [GOSCHOOL(x) ⇒ CANREAD(x)]
  
  $\neg$GOSCHOOL($x_1$) ∨ CANREAD($x_1$)    $\neg$CANREAD($x_2$) ∨ LITERATE($x_2$)
  
  mgu $s = \{x_2|x_1\}$
  
  $\neg$GOSCHOOL($x_1$) ∨ LITERATE($x_1$)
- ### Resolution-Refutation: Example (I)
  
  1. If a unit is easy, there are some students who are enrolled in it who are happy
  2. If a unit has a final exam, no students that are enrolled in it are happy
  3. Prove that if a unit has a final exam, the unit is not easy
  
  $\forall u$ [EASY(u) ⇒ $\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  $\forall u$ [HASFINAL(u) ⇒ ¬$\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  or
  
  $\forall u$ [HASFINAL(u) ⇒ $\forall s$ [ENROLLED(s, u) ⇒ ¬HAPPY(s)]]
  
  $\forall u$ [HASFINAL(u) ⇒ ¬EASY(u)]
- ### Resolution-Refutation: Example (II)
  
  **Converting to clauses:**
  
  1. $\forall u$ [EASY(u) ⇒ $\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  Eliminate ⇒: $\forall u$ [¬EASY(u) ∨ $\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  Eliminate ∃: $\forall u$ [¬EASY(u) ∨ [ENROLLED(g(u), u) ∧ Happy(g(u))]]
  
  Turn into CNF: $\forall u$ [[¬EASY(u) ∨ ENROLLED(g(u), u)] ∧ [¬EASY(u) ∨ HAPPY(g(u))]]
  
  Eliminate ∀: [¬EASY(u) ∨ ENROLLED(g(u), u)] ∧ [¬EASY(u) ∨ HAPPY(g(u))]
  
  Eliminate ∧ and standardize variables apart:
- 1.1 ¬EASY($u_1$) ∨ ENROLLED(g($u_1$), $u_1$)
- 1.2 ¬EASY($u_2$) ∨ HAPPY(g($u_2$))
- ### Resolution-Refutation: Example (III)
  
  2. $\forall u$ [HASFINAL(u) ⇒ ¬$\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  Eliminate ⇒: $\forall u$ [¬HASFINAL(u) ∨ ¬$\exists s$ [ENROLLED(s, u) ∧ HAPPY(s)]]
  
  Reduce scope of ¬: $\forall u$ [¬HASFINAL(u) ∨ $\forall s \neg$[ENROLLED(s, u) ∧ HAPPY(s)]]
  
  $\forall u$ [¬HASFINAL(u) ∨ $\forall s$ [¬ENROLLED(s, u) ∨ ¬HAPPY(s)]]
  
  Move ∀ to the front: $\forall u \forall s$ [¬HASFINAL(u) ∨ ¬ENROLLED(s, u) ∨ ¬HAPPY(s)]
  
  Eliminate ∀: ¬HASFINAL(u) ∨ ¬ENROLLED(s, u) ∨ ¬HAPPY(s)
  
  Standardize variables apart: ¬HASFINAL($u_3$) ∨ ¬ENROLLED($s_3$, $u_3$) ∨ ¬HAPPY($s_3$)
- ### Resolution-Refutation: Example (IV)
  
  **Using resolution to prove statement 3**
  
  Negate the goal:
  3'. ¬$\forall u$ [HASFINAL(u) ⇒ ¬EASY(u)]
  
  Eliminate ⇒: ¬$\forall u$ [¬HASFINAL(u) ∨ ¬EASY(u)]
  
  Reduce scope of ¬: $\exists u$ [HASFINAL(u) ∧ EASY(u)]
  
  Eliminate ∃: HASFINAL(A) ∧ EASY(A)
  
  Eliminate ∧:
- 3'.1 HASFINAL(A)
- 3'.2 EASY(A)
- ### Resolution-Refutation: Example (V)
  
  [Resolution proof tree showing steps to derive NIL]
- ### Resolution-Refutation Strategies
- **Unit preference**: prefer resolutions where one of the sentences is a unit clause (single literal)
	- **Unit resolution**: every resolution must involve a unit clause
- **Set of support**: every resolution step should involve at least one element of a special set of clauses, the set of support. The resolvent is added to the set of support
	- E.g., set of support = {¬Q}
- **Input resolution**: every resolution combines one of the original input sentences (from the KB or the query) with another sentence
- **Subsumption**: eliminates all sentences that are subsumed by (more specific than) an existing sentence in the KB
- ### Uses of FOL in AI
  
  **Theorem proving:**
  1. ON(C, A)
  2. ONTable(A), ONTable(B)
  3. CLEAR(C), CLEAR(B)
  4. $\forall x$ [CLEAR(x) ⇒ ¬$\exists y$ ON(y, x)]
  5. Goal: Prove ¬$\exists y$ ON(y, C)
  
  **Question answering:**
  1. MANAGER(Purchasing-dept., John-Jones)
  2. WORKSIN(Purchasing-dept., Joe-Smith)
  3. $\forall x \forall y \forall z$ [WORKSIN(x, y) ∧ MANAGER(x, z)] ⇒ BOSSOF(y, z)
  4. Goal: Who is the boss of Joe Smith?
   $\exists x$ BOSSOF(Joe-Smith, x)
  
  **Planning**
- ### Theorem Proving: Example
  
  1. ON(C, A)
  2. (a) ONTABLE(A), (b) ONTABLE(B)
  3. (a) CLEAR(C), (b) CLEAR(B)
  4. $\forall x$ [CLEAR(x) ⇒ ¬$\exists y$ ON(y, x)]
   ¬CLEAR($x_1$) ∨ ¬ON(y, $x_1$)
  5. Goal: Prove ¬$\exists y$ ON(y, C)
  
  Negate goal: $\exists y$ ON(y, C) → ON(M, C)
  
  [Resolution proof showing derivation of NIL]
- ### Question Answering: Example
  
  [Resolution proof for finding boss of Joe-Smith]
- ### Reading
  
  Russell, S. and Norvig, P. (2010), *Artificial Intelligence – A Modern Approach* (3rd edition), Prentice Hall
- Chapter 6, Sections 6.1-6.5
- Chapter 8
- Chapter 9, Sections 9.1, 9.2 and 9.5
  
  ---
  
  This completes Weeks 4-7. Would you like me to continue with the remaining weeks (Probability, Bayesian Networks, and Machine Learning)?
  
  | +a | 0.8 | 0.1 |
  | ¬b | 0.2 | 0.9 |
  
  | +b | 0.1 |
  |---|---|
  | ¬b | 0.9 |
  
  →
  
  | | +b | ¬b |
  |---|---|---|
  | +a | 8/17 | 9/17 |
  
  Pr(+a, B) Pr(A | B) Pr(B | +a)
  
  **1. Start / Select** **2. Join on B** **3. Normalize**
  
  **Pr(B | +a) = ?**
  
  $$\text{Pr}(B | +a) = \frac{\text{Pr}(+a, B)}{\text{Pr}(+a)}$$
  
  Bayes rule:
- ### Summary: BNs
- Bayes nets compactly encode joint distributions
- BNs are a natural way to represent conditional independence information
	- **qualitative:** links between nodes – independencies of distributions can be deduced from a BN by D-separation
	- **quantitative:** conditional probability tables (CPTs)
- **BN inference:**
	- computes the probability of query variables given evidence
	- is flexible: we can enter evidence about any node and update beliefs in other nodes
	- using variable elimination is better than using enumeration
- ### Reading and Software
  
  **Reading:**
- Russell, S. and Norvig, P. (2010), *Artificial Intelligence – A Modern Approach* (3rd ed), Prentice Hall
	- Chapters 14.1-14.4.1, 16.1-2, 16.5
- Korb, K. and Nicholson, A. (2010), *Bayesian Artificial Intelligence* (2nd ed), Chapman and Hall
	- Chapters 1, 2, 3.1-3.3 and 4.1-4.4
	  
	  **Software:**
- Netica – http://www.norsys.com/
  
  ---
  
  This completes the transcription through Week 11 (Bayesian Networks). The document ends here in the provided materials. The lecture notes conclude with Bayesian Networks, covering probability, reasoning under uncertainty, and exact inference methods.
  
  **Summary of Complete Transcription:**
- Week 1: Introduction to AI and Intelligent Agents
- Weeks 2-3: Problem Solving as Search (uninformed and informed strategies)
- Weeks 4-5: Knowledge Representation (Propositional and First-Order Logic)
- Weeks 6-7: Adversarial Search (game trees, minimax, alpha-beta pruning)
- Weeks 8-9: Probability and Reasoning Under Uncertainty
- Weeks 10-11: Bayesian Networks (representation, inference, decision networks)